{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
    "\n",
    "model_urls = {\n",
    "    'squeezenet1_0': '../input/fourthpth/visionmaster/visionmaster/torchvision/models/squeezenet1_0-a815701f.pth',\n",
    "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                   kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, version='1_0', num_classes=1000):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if version == '1_0':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(96, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        elif version == '1_1':\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        else:\n",
    "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
    "            # FIXME: squeezenet1_x() functions\n",
    "            # FIXME: This checking is not done for the other models\n",
    "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
    "                             \"1_0 or 1_1 expected\".format(version=version))\n",
    "\n",
    "        # Final convolution is initialized differently from the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "def _squeezenet(version, pretrained, **kwargs):\n",
    "    model = SqueezeNet(version, **kwargs)\n",
    "    if pretrained:\n",
    "        arch = 'squeezenet' + version\n",
    "        state_dict = torch.load(model_urls[arch])\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def squeezenet1_0(pretrained=False,**kwargs):\n",
    "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
    "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
    "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _squeezenet('1_0', pretrained, **kwargs)\n",
    "\n",
    "\n",
    "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SqueezeFeatures, self).__init__()\n",
    "        base_model =  squeezenet1_0(pretrained=True)\n",
    "\n",
    "        self.seq1 = nn.Sequential(base_model.features \n",
    "                                  )\n",
    "        self.out_channels = 512\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq1(x)\n",
    "\n",
    "        return x\n",
    "backbone = SqueezeFeatures()\n",
    "backbone.out_channels = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_squeeze(pretrained=False, progress=True,\n",
    "                            num_classes=2, pretrained_backbone=True,\n",
    "                             trainable_backbone_layers=3, **kwargs):\n",
    "    assert trainable_backbone_layers <= 5 and trainable_backbone_layers >= 0\n",
    "    # dont freeze any layers if pretrained model or backbone is not used\n",
    "    if not (pretrained or pretrained_backbone):\n",
    "        trainable_backbone_layers = 5\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = fasterrcnn_squeeze(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"../input/fourthpth/results/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): SqueezeFeatures(\n",
       "    (seq1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (3): Fire(\n",
       "          (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Fire(\n",
       "          (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Fire(\n",
       "          (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (7): Fire(\n",
       "          (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (8): Fire(\n",
       "          (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (9): Fire(\n",
       "          (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (10): Fire(\n",
       "          (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "        (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "        (12): Fire(\n",
       "          (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (squeeze_activation): ReLU(inplace=True)\n",
       "          (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (expand1x1_activation): ReLU(inplace=True)\n",
       "          (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (expand3x3_activation): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(save_model_path, f\"best_model_epoch.pth\")))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetArguments:\n",
    "    data_dir: Path\n",
    "    images_lists_dict: dict\n",
    "    labels_csv_file_name: str\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderArguments:\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    dataset_arguments: DatasetArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = Path(\"/kaggle/input/global-wheat-detection/\")\n",
    "unlabeled_generated_images_path = Path(f\"/kaggle/input/global-wheat-detection/{phase}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_file_names(directory):\n",
    "    _, _, files = next(os.walk(directory))\n",
    "    return files\n",
    "test_file_names = get_images_file_names(unlabeled_generated_images_path)\n",
    "test_file_names = [x.split(\".\")[0] for x in test_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_lists_dict = {\n",
    "    \"test\": test_file_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataset_arguments = DatasetArguments(\n",
    "    data_dir=root_data_dir,\n",
    "    images_lists_dict=images_lists_dict,\n",
    "    labels_csv_file_name=\"sample_submission.csv\",\n",
    ")\n",
    "predict_dataloaders_arguments = DataLoaderArguments(\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "    dataset_arguments=prediction_dataset_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_set():\n",
    "    transforms_dict = {\n",
    "        'test': get_test_transforms()\n",
    "    }\n",
    "    return transforms_dict\n",
    "\n",
    "\n",
    "def get_test_transforms():\n",
    "    return Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_root_directory, images_list, labels_csv_file_name, phase, transforms):\n",
    "        super(ObjectDetectionDataset).__init__()\n",
    "        self.images_root_directory = images_root_directory\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "        self.images_list = images_list\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            self.labels_dataframe = pd.read_csv(os.path.join(images_root_directory, labels_csv_file_name))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = {\n",
    "            \"local_image_id\": None,\n",
    "            \"image_id\": None,\n",
    "            \"labels\": None,\n",
    "            \"boxes\": None,\n",
    "            \"area\": None,\n",
    "            \"iscrowd\": None\n",
    "        }\n",
    "\n",
    "        image_id = self.images_list[item]\n",
    "        image_path = os.path.join(self.images_root_directory,\n",
    "                                  \"train\" if self.phase in [\"train\", \"val\"] else \"test\",\n",
    "                                  image_id + \".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        sample[\"local_image_id\"] = image_id\n",
    "        sample[\"image_id\"] = torch.tensor([item])\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            boxes = self.labels_dataframe[self.labels_dataframe.image_id == image_id].bbox.values.tolist()\n",
    "            boxes = [eval(box_i) for box_i in boxes]\n",
    "            areas = _areas(boxes)\n",
    "            boxes = _adjust_boxes_format(boxes)\n",
    "\n",
    "            sample[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            sample[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            sample[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            sample[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        if self.transforms is not None:\n",
    "            sample[\"image\"] = image\n",
    "            if self.phase in [\"train\", \"val\"]:\n",
    "                transformed_sample = self.transforms(image=sample[\"image\"],\n",
    "                                                     bboxes=sample[\"boxes\"],\n",
    "                                                     labels=sample[\"labels\"])\n",
    "                sample[\"boxes\"] = torch.as_tensor(transformed_sample[\"bboxes\"], dtype=torch.float32)\n",
    "            else:\n",
    "                transformed_sample = self.transforms(image=sample[\"image\"])\n",
    "            image = transformed_sample[\"image\"]\n",
    "            del sample[\"image\"]\n",
    "        return image, sample\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(arguments):\n",
    "    dataset = ObjectDetectionDataset(arguments.data_dir,\n",
    "                                     arguments.images_lists_dict[arguments.phase],\n",
    "                                     arguments.labels_csv_file_name,\n",
    "                                     arguments.phase,\n",
    "                                     arguments.transforms)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_dataloader(arguments, input_size):\n",
    "    data_transforms = transform_set()\n",
    "    batch_size = arguments.batch_size\n",
    "    num_workers = arguments.num_workers\n",
    "    arguments.dataset_arguments.phase = phase\n",
    "    arguments.dataset_arguments.transforms = data_transforms[\"test\"]\n",
    "    image_datasets = create_dataset(arguments.dataset_arguments)\n",
    "    dataloader = DataLoader(image_datasets, batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            pin_memory=True,\n",
    "                            num_workers=num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_prediction_dataloader(predict_dataloaders_arguments, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_threshold=0.45\n",
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    }
   ],
   "source": [
    "for images, sample in dataloader:\n",
    "    image_ids = [x[\"local_image_id\"] for x in sample]\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "\n",
    "        boxes = outputs[i]['boxes'].data.cpu().numpy()\n",
    "        scores = outputs[i]['scores'].data.cpu().numpy()\n",
    "        \n",
    "        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n",
    "        scores = scores[scores >= detection_threshold]\n",
    "        image_id = image_ids[i]\n",
    "        \n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        \n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores) if boxes.shape[0] > 0 else \"\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2fd875eaa</td>\n",
       "      <td>0.9862 106 584 142 81 0.9774 461 355 127 98 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9713 486 578 96 133 0.9525 83 598 103 165 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.9791 109 843 157 97 0.9738 234 638 91 165 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53f253011</td>\n",
       "      <td>0.9797 226 835 119 101 0.9433 15 31 141 109 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cb8d261a3</td>\n",
       "      <td>0.8716 258 773 128 72 0.8562 759 715 79 81 0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  2fd875eaa  0.9862 106 584 142 81 0.9774 461 355 127 98 0....\n",
       "1  cc3532ff6  0.9713 486 578 96 133 0.9525 83 598 103 165 0....\n",
       "2  51b3e36ab  0.9791 109 843 157 97 0.9738 234 638 91 165 0....\n",
       "3  53f253011  0.9797 226 835 119 101 0.9433 15 31 141 109 0....\n",
       "4  cb8d261a3  0.8716 258 773 128 72 0.8562 759 715 79 81 0.8..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
